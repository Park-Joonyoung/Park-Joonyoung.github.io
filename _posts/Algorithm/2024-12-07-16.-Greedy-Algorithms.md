---
title: 16. Greedy Algorithms
# description: Short summary of the post
date: 2024-12-07 16:04
categories: [Computer Science, Algorithm]
tags: [greedy-algorithm, activity-selection-problem, greedy-choice-property, optimal-substructure, huffman-codes]     # TAG names should always be lowercase
math: true
pin: false
---

A greedy algorithm always makes the choice that looks best at the moment.
It makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution.

## 16.1 An activity-selection problem

Suppose we have a set $$ S = \{ a_1, \ a_2, \dots, \ a_n \} $$ of $$ n $$ proposed activities that wish to use a resource.
Each activity $$ a_i $$ has a start time $$ s_i $$ and a finish time $$ f_i $$, where $$ 0 \le s_i < f_i < \infty $$.
If selected, activity $$ a_i $$ takes place during the half-open time interval $$ [s_i, \ f_i) $$.
Activities $$ a_i $$ and $$ a_j $$ are compatible if the intervals $$ [s_i, \ f_i) $$ and $$ [s_j, \ f_j) $$ do not overlap.
In the activity-selection problem, we wish to select a maximum-size subset of mutually compatible activities.
Assume that the activities are sorted in monotonically increasing order of finish time:

$$
\begin{align*}
    f_1 \le f_2 \le \dots \le f_n
\end{align*}
\label{eq:1}
\tag{16.1}
$$

### The optimal substructure of the activity-selection problem

Denote by $$ S_{ij} $$ the set of activities that start after activity $$ a_i $$ finishes and that finish before activity $$ a_j $$ starts.
Suppose $$ A_{ij} $$ is a maximum set of mutually compatible activities in $$ S_{ij} $$, which includes some activity $$ a_k $$.
We assume that $$ a_k $$ is included in an optimal solution.
Let $$ A_{ik} = A_{ij} \cap S_{ik} $$ and $$ A_{kj} = A_{ij} \cap S_{kj} $$, then it turns out that $$ A_{ij} = A_{ik} \cup \{ a_k \} \cup A_{kj} $$.
Therefore, $$ A_{ij} $$ consists of $$ |A_{ij}| = |A_{ik}| + |A_{kj}| + 1 $$ activities.

The cut-and-paste argument shows that the optimal solution $$ A_{ij} $$ includes optimal solutions to the two subproblems for $$ S_{ik} $$ and $$ S_{kj} $$.
Suppose there is a set $$ A'_{kj} $$ of mutually compatible activities in $$ S_{kj} $$ where $$ |A'_{kj}| > |A_{kj}| $$, then $$ |A_{ik}| + |A'_{kj}| + 1 > |A_{ik}| + |A_{kj}| + 1 = |A_{ij}| $$ holds, which contradicts the assumption that $$ A_{ij} $$ is an optimal solution.
A symmetric argument applies to the activities in $$ S_{ik} $$.

This way of characterizing optimal substructure suggests that we might solve the activity-selection problem by dynamic programming.
Denote the size of an optimal solution for the set $$ S_{ij} $$ by $$ c[i, \ j] $$, then we would have the recurrence

$$
\begin{align*}
    c[i, \ j] =
    \begin{cases}
        \displaystyle 0                                                     & \text{if } S_{ij} = \emptyset \\
        \displaystyle \max_{a_k \in S_{ij}} \{ c[i, \ k] + c[k, \ j] + 1 \} & \text{if } S_{ij} \neq \emptyset
    \end{cases}
\end{align*}
\label{eq:2}
\tag{16.2}
$$

We could then develop a recursive algorithm and memoize it, or we could work bottom-up and fill in table entries as we go along.
However, we would be overlooking another important characteristic of the activity-selection problem.

### Making the greedy choice

Intuition suggest that we should choose an activity that leaves the resource available for as many other activities as possible.
Now, of the activities we end up choosing, one of them must be the first one to finish.
Therefore, we choose the activity in $$ S $$ with the earliest finish time, since that would leave the resource available for as many of the activities that follow it as possible.
Because the activities are sorted in monotonically increasing order by finish time, the greedy choice is activity $$ a_1 $$.

If we make the greedy choice, we have only one remaining subproblem to solve: finding activities that start after $$ a_1 $$ finishes.
We have that $$ s_1 < f_1 $$, and $$ f_1 $$ is the earliest finish time of any activity, and therefore no activity can have a finish time less than or equal to $$ s_1 $$.
Thus, all activities that are compatible with activity $$ a_1 $$ must start after $$ a_1 $$ finishes.  
Furthermore, we have already established that the activity-selection problem exhibits optimal substructure.
Let $$ S_k = \{ a_i \in S : s_i \ge f_k \} $$ be the set of activities that start after activity $$ a_k $$ finishes.
If we make the greedy choice of activity $$ a_1 $$, then $$ S_1 $$ remains as the only subproblem to solve.
(We sometimes refer to the sets $$ S_k $$ as subproblems rather than as just sets of activities.)
Optimal substructure tells us that if $$ a_1 $$ is in the optimal solution, then an optimal solution to the original problem consists of activity $$ a_1 $$ and all the activities in an optimal solution to the subproblem $$ S_1 $$.

### Theorem 16.1

Consider any nonempty subproblem $$ S_k $$, and let $$ a_m $$ be an activity in $$ S_k $$ with the earliest finish time.
Then $$ a_m $$ is included in some maximum-size subset of mutually compatible activities of $$ S_k $$.

**Proof**  
Let $$ A_k $$ be a maximum-size subset of mutually compatible activities in $$ S_k $$, and let $$ a_j $$ be the activity in $$ A_k $$ with the earliest finish time.
If $$ a_j = a_m $$, we are done.
If $$ a_j \neq a_m $$, let the set $$ A'_k = A_k - \{ a_j \} \cup \{ a_m \} $$ be $$ A_k $$ but substituting $$ a_m $$ for $$ a_j $$.
The activities in $$ A'_k $$ are disjoint, which follows because the activities in $$ A_k $$ are disjoint, $$ a_j $$ is the first activity in $$ A_k $$ to finish, and $$ f_m \le f_j $$.
Since $$ |A'_k| = |A_k| $$, we conclude that $$ |A'_k| $$ is a maximum-size subset of mutually compatible activities of $$ S_k $$, and it includes $$ a_m $$. $$ \blacksquare $$

We can repeatedly choose the activity that finishes first, keep only the activities compatible with this activity, and repeat until no activities remain.
An algorithm to solve the activity-selection problem does not need to work bottom-up, like a table-based dynamic-programming algorithm.
Instead, it can work top-down, choosing an activity to put into the optimal solution and then solving the subproblem of choosing activities from those that are compatible with those already chosen.
Greedy algorithms typically have this top-down design: make a choice and then solve a subproblem, rather than the bottom-up technique of solving subproblems before making a choice.

### A recursive greedy algorithm

>RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$)  
>&nbsp;1&nbsp; $$ m = k + 1 $$  
>&nbsp;2&nbsp; while $$ m \le n $$ and $$ s[m] < f[k] $$  
>&nbsp;3&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m = m + 1 $$  
>&nbsp;4&nbsp; if $$ m \le n $$  
>&nbsp;5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ \{ a_m \} \ \cup $$ RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$)  
>&nbsp;6&nbsp; else return $$ \emptyset $$

In a given recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$), the while loop of lines 2–3 looks for the first activity in $$ S_k $$ to finish.
The loop examines $$ a_{k + 1}, \ a_{k + 2}, \dots , \ a_{n} $$, until it finds the first activity $$ a_m $$ that is compatible with $$ a_k $$; such an activity has $$ s_m \ge f_k $$.
If the loop terminates because it finds such an activity, line 5 returns the union of $$ \{ a_m \} $$ and the maximum-size subset of $$ S_m $$ returned by the recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$).
Alternatively, the loop may terminate because $$ m > n $$, in which case we have examined all activities in $$ S_k $$ without finding one that is compatible with $$ a_k $$.
In this case, $$ S_k = \emptyset $$, and so the procedure returns $$ \emptyset $$ in line 6.

Assuming that the activities have already been sorted by finish times, the running time of the call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ 0, \ n $$) is $$ \Theta(n) $$, which we can see as follows.
Over all recursive calls, each activity is examined exactly once in the while loop test of line 2.
In particular, activity $$ a_i $$ is examined in the last call made in which $$ k < i $$.

### An iterative greedy algorithm

>GREEDY-ACTIVITY-SELECTOR($$ s, \ f $$)  
>&nbsp;1&nbsp; $$ n = s.length $$  
>&nbsp;2&nbsp; $$ A = \{ a_1 \} $$  
>&nbsp;3&nbsp; $$ k = 1 $$  
>&nbsp;4&nbsp; for $$ m = 2 $$ to $$ n $$  
>&nbsp;5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ s[m] \ge f[k] $$  
>&nbsp;6&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ A = A \cup \{ a_m \} $$  
>&nbsp;7&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ k = m $$  
>&nbsp;8&nbsp; return $$ A $$

The variable $$ k $$ indexes the most recent addition to $$ A $$, corresponding to the activity $$ a_k $$ in the recursive version.
Since we consider the activities in order of monotonically increasing finish time, $$ f_k $$ is always the maximum finish time of any activity in $$ A $$.
That is,

$$
\begin{align*}
    f_k = \max \{ f_i : a_i \in A \}
\end{align*}
\label{eq:3}
\tag{16.3}
$$

Lines 2–3 select activity $$ a_1 $$, initialize $$ A $$ to contain just this activity, and initialize $$ k $$ to index this activity.
The for loop of lines 4–7 finds the earliest activity in $$ S_k $$ to finish.
The loop considers each activity $$ a_m $$ in turn and adds $$ a_m $$ to $$ A $$ if it is compatible with all previously selected activities.
To see whether activity $$ a_m $$ is compatible with every activity in $$ A $$, by equation \eqref{eq:3}, it suffices to check that $$ s_m \ge f_k $$.
If activity $$ a_m $$ is compatible, then lines 6–7 add activity $$ a_m $$ to $$ A $$ and set $$ k $$ to $$ m $$.  
Like the recursive version, GREEDY-ACTIVITY-SELECTOR schedules a set of $$ n $$ activities in $$ \Theta(n) $$ time, assuming that the activities were already sorted initially by their finish times.

## 16.2 Elements of the greedy strategy

A greedy algorithm obtains an optimal solution to a solution to a problem by making a sequence of choices.
At each decision point, the algorithm makes the choice that seems best at the moment.
This heuristic strategy does not always produce an optimal solution, but sometimes, it does.
We design greedy algorithms according to the following sequence of steps:

1. Cast the optimization problem as one in which we make a choice and are left with one subproblem to solve.
2. Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.
3. Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if we combine an optimal solution to the subproblem with the greedy choice we have made, we arrive at an optimal solution to the original problem.

There are two key ingredients that the problem can be solved by using a greedy algorithm: the greedy-choice property and optimal substructure.

### Greedy-choice property

The greedy-choice property is that we can assemble a globally optimal solution by making locally optimal greedy choices.
In other words, when we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems.  
Here is where greedy algorithms differ from dynamic programming.
In dynamic programming, the choice we make depends on the solutions to subproblems.
Consequently, we typically solve dynamic-programming problems in a bottom-up manner, progressing from smaller subproblems to larger ones.
In a greedy algorithm, we make whatever choice seems best at the moment and then solve the subproblem that remains.
The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems.
A dynamic programming algorithm proceeds bottom up, whereas a greedy strategy usually progresses in a top-down fashion, making one greedy choice after another, reducing each given problem instance to a smaller one.

### Optimal substructure

A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solution to subproblems.
This property is a key ingredient of assessing the applicability of dynamic programming as well as greedy algorithms.
As an example of optimal substructure in Section 16.1, if an optimal solution to subproblem $$ S_{ij} $$ includes an activity $$ a_k $$, then it must also contain optimal solutions to the subproblems $$ S_{ik} $$ and $$ S_{kj} $$.
Based on this observation of optimal substructure, we were able to devise the recurrence \eqref{eq:2} that described the value of an optimal solution.  
We usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms.
We can assume that we arrived at a subproblem by having made the greedy choice in the original problem.
All we really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem.
This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step producese an optimal solution.

## 16.3 Huffman codes

Suppose we have a 100,000-character data file that we wish to store compactly.
We observe that the characters in the file occur with the frequencies given as below. That is, only 6 different characters appear, and the character **a** occurs 45,000 times.

| |**a**|**b**|**c**|**d**|**e**|**f**|
|---|:---:|:---:|:---:|:---:|:---:|:---:|
|Frequency (in thousands)|45|13|12|6|9|5|
|Fixed-length codeword|000|001|010|011|100|101|
|Variable-length codeword|0|101|100|111|1101|1100|

Here, we consider the problem of designing a binary character code (or code for short) in which each character is represented by a unique binary string, which is called a codeword.
If we use a fixed-length codeword, we need 3 bits to represent 6 characters.
This method requires 300,000 bits to code the entire file.  
A variable-length code can do considerably better that a fixed-length code, by giving frequent characters short codewords and infrequent characters long codewords.
The table above shows such a code; here the 1-bit string 0 represents **a**, and the 4-bit string 1100 represents **f**.
This code requires

$$
\begin{align*}
    (45 \cdot 1 + 13 \cdot 3 + 12 \cdot 3 + 16 \cdot 3 + 9 \cdot 4 + 5 \cdot 4) \cdot 1000 = 224,000 \text{ bits}
\end{align*}
$$

to represent the file, a savings of approximately 25%.
In fact, this is an optimal character code for this file.

### Prefix codes

We consider only codes in which no codewords is also a prefix of some other codeword.
Such codes are called prefix codes.
A prefix code can always achieve the optimal data compression among any character code, and so we suffer no loss of generality by restricting our attention to prefix codes.

Prefix codes are desirable because they simplify decoding.
Since no codeword is a prefix of any other, the codeword that begins an encoded file is unambiguous.
For example, the string 001011101 parses uniquely as $$ 0 \cdot 0 \cdot 101 \cdot 1101 $$, which decodes to **aabe**.

A binary tree whose leaves are the given characters provides a convenient representation for the prefix code.
We interpret the binary codeword for a character as the simple path from the root to that character, where 0 means "go to the left child" and 1 means "go to the right child."
Figure 16.1 shows the trees for the two codes of our example.
Note that these are not binary search trees, since the leaves need not appear in sorted order and internal nodes do not contain character keys.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.1.png){: width="700" height="400" }
_**Figure 16.1** **(a)** The tree corresponding to the fixed-length code. **(b)** The tree corresponding to the optimal prefix code._

An optimal code for a file is always represented by a full binary tree, in which every nonleaf node has two children.
The fixed-length code is not optimal since its tree is not a full binary tree: it contains codewords beginning $$ 10 \dots $$ but none beginning $$ 11 \dots $$.
Since we can now restrict our attention to full binary trees, we can say that if $$ C $$ is the alphabet from which the characters are drawn and all character frequencies are positive, then the tree for an optimal prefix code has exactly $$ |C| $$ leaves, one for each letter of the alphabet, and exactly $$ |C| - 1 $$ internal nodes.

Given a tree $$ T $$ corresponding to a prefix code, we can easily compute the number of bits required to encode a file.
For each character $$ c $$ in the alphabet $$ C $$, let the attribute $$ c.freq $$ denote the frequency of $$ c $$ in the file and let $$ d_T(c) $$ denote the depth of $$ c $$'s leaf in the tree.
Note that $$ d_T(c) $$ is also the length of the codeword for character $$ c $$.
The number of bits required to encode a file is thus

$$
\begin{align*}
    B(T) = \sum_{c \in C}{c.freq \cdot d_T(c)}
\end{align*}
\label{eq:4}
\tag{16.4}
$$

which we define as the cost of the tree $$ T $$.

### Constructing a Huffman code

In the pseudocode that follows, the algorithm builds the tree $$ T $$ corresponding to the optimal code in a bottom-up manner.
It begins with $$ |C| $$ leaves and performs a sequence of $$ |C| - 1 $$ merging operations to create the final tree.
The algorithm uses a min-priority queue $$ Q $$, keyed on the $$ freq $$ attribute, to identify the two least-frequent objects to merge together.
When we merge two objects, the result is a new object whose frequency is the sum of the frequencies of the two objects that were merged.

>HUFFMAN($$ C $$)  
>&nbsp;1&nbsp; $$ n = |C| $$  
>&nbsp;2&nbsp; $$ Q = C $$  
>&nbsp;3&nbsp; for $$ i = 1 $$ to $$ n - 1 $$  
>&nbsp;4&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;allocate a new node $$ z $$  
>&nbsp;5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.left = x = $$ EXTRACT-MIN($$ Q $$)  
>&nbsp;6&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.right = y = $$ EXTRACT-MIN($$ Q $$)  
>&nbsp;7&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.freq = x.freq + y.freq $$  
>&nbsp;8&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;INSERT($$ Q, \ z $$)  
>&nbsp;9&nbsp; return EXTRACT-MIN($$ Q $$)

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.2.png){: width="700" height="400" }
_**Figure 16.2** The construction of a Huffman code._

Line 2 initializes the min-priority queue $$ Q $$ with the characters in $$ C $$.
The for loop in lines 3–8 repeatedly extracts the two nodes $$ x $$ and $$ y $$ of lowest frequency from the queue, replacing them in the queue with a new node $$ z $$, representing their merger.
The frequency of $$ z $$ is computed as the sum of the frequencies of $$ x $$ and $$ y $$ in line 7.
The node $$ z $$ has $$ x $$ and $$ y $$ as its children.
(The order of $$ x $$ and $$ y $$ is arbitrary; switching the left and right child of any node yields a different code of the same cost.)
After $$ n - 1 $$ mergers, line 9 returns the one node left in the queue, which is the root of the code tree.

To analyze the running time of Huffman's algorithm, we assume that $$ Q $$ is implemented as a binary min-heap.
For a set $$ C $$ of $$ n $$ characters, we can initialize $$ Q $$ in $$ O(n) $$ time using the BUILD-MIN-HEAP procedure discussed in Section 6.3.
The for loop in lines 3–8 executes exactly $$ n - 1 $$ times, and since each heap operation requires $$ O(\lg n) $$, the loop contributes $$ O(n \lg n) $$ to the running time.
Thus, the total running time of HUFFMAN on a set of $$ n $$ characters is $$ O(n \lg n) $$.

### Correctness of Huffman's algorithm

### Lemma 16.2 (Greedy-choice property of Huffman's algorithm)

Let $$ C $$ be an alphabet in which each character $$ c \in C $$ has frequency $$ c.freq $$.
Let $$ x $$ and $$ y $$ be two characters in $$ C $$ having the lowest frequencies.
Then there exists an optimal prefix code for $$ C $$ in which the codewords for $$ x $$ and $$ y $$ have the same length and differ only in the last bit.

**Proof**  
Let $$ a $$ and $$ b $$ be two characters that are sibling leaves of maximum depth in $$ T $$.
WLOG we assume that $$ a.freq \le b.freq $$ and $$ x.freq \le y.freq $$.
Since $$ x.freq $$ and $$ y.freq $$ are the two lowest leaf frequencies, in order, and $$ a.freq $$ and $$ b.freq $$ are two arbitrary frequencies, in order, we have $$ x.freq \le a.freq $$ and $$ y.freq \le b.freq $$.
If $$ x.freq = b.freq $$, then we would have $$ a.freq = b.freq = x.freq = y.freq $$, and the lemma would be trivially true.
Thus, we will assume that $$ x.freq \neq b.freq $$, which means that $$ x \neq b $$.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.3.png){: width="700" height="200" }
_**Figure 16.3**_

As Figure 16.3 shows, we exchange the positions in $$ T $$ of $$ a $$ and $$ x $$ to produce a tree $$ T' $$, and then we exchange the positions in $$ T' $$ of $$ b $$ and $$ y $$ to produce a tree $$ T'' $$ in which $$ x $$ and $$ y $$ are sibling leaves of maximum depth.
(Note that if $$ x = b $$ but $$ y \neq a $$, then tree $$ T'' $$ does not have $$ x $$ and $$ y $$ as sibling leaves of maximum depth.)
By equation \eqref{eq:4}, the difference in cost between $$ T $$ and $$ T' $$ is

$$
\begin{align*}
    B(T) - B(T') &= \sum_{c \in C}{c.freq \cdot d_T(c)} - \sum_{c \in C}{c.freq \cdot d_{T'}(c)} \\
                 &= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T'}(x) - a.freq \cdot d_{T'}(a) \\
                 &= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_T(a) - a.freq \cdot d_T(x) \\
                 &= (a.freq - x.freq)(d_T(a) - d_T(x)) \\
                 &\ge 0
\end{align*}
$$

Observe that exchanging $$ x $$ and $$ a $$ does not increase the cost.
Similarly, exchanging $$ y $$ and $$ b $$ does not increase the cost, and so $$ B(T') - B(T'') $$ is nonnegative.
Thus, $$ B(T'') \le B(T) $$, and since $$ T $$ is optimal, we have $$ B(T) \le B(T'') $$, which implies $$ B(T'') = B(T) $$.
Therefore, $$ T'' $$ is an optimal tree in which $$ x $$ and $$ y $$ appear as sibling leaves of maximum depth, from which the lemma follows. $$ \blacksquare $$

Lemma 16.2 implies that the process of building up an optimal tree by mergers can, WLOG begin with the greedy choice of mergine together those two characters of lowest frequency.

### Lemma 16.3 (Optimal-substructure property of Huffman's algorithm)

Let $$ C $$ be a given alphabet with frequency $$ c.freq $$ defined for each character $$ c \in C $$.
Let $$ x $$ and $$ y $$ be two characters in $$ C $$ with minimum frequency.
Let $$ C' $$ be the alphabet $$ C $$ with the characters $$ x $$ and $$ y $$ removed and a new character $$ z $$ added, so that $$ C' = C - \{ x, \ y \} \cup \{ z \} $$.
Define $$ freq $$ for $$ C' $$ as for $$ C $$, except that $$ z.freq = x.freq + y.freq $$.
Let $$ T' $$ be any tree representing an optimal prefix code for the alphabet $$ C' $$.
Then the tree $$ T $$, obtained from $$ T' $$ by replacing the leaf node for $$ z $$ with an internal node having $$ x $$ and $$ y $$ as children, represents an optimal prefix code for the alphabet $$ C $$.

**Proof**  
For each character $$ c \in C - \{ x, \ y \} $$, we have that $$ d_T(c) = d_{T'}(c) $$, and hence $$ c.freq \cdot d_T(c) = c.freq \cdot d_{T'}(c) $$.
Since $$ d_T(x) = d_T(y) = d_{T'}(z) + 1 $$, we have

$$
\begin{align*}
    x.freq \cdot d_T(x) + y.freq \cdot d_T(y) &= (x.freq + y.freq)(d_{T'}(z) + 1) \\
                                              &= z.freq \cdot d_{T'}(z) + (x.freq + y.freq)
\end{align*}
$$

from which we conclude that

$$
\begin{align*}
    B(T) = B(T') + x.freq + y.freq
\end{align*}
$$

or, equivalently,

$$
\begin{align*}
    B(T') = B(T) - x.freq - y.freq
\end{align*}
$$

We now prove the lemma by contradiction.
Suppose that $$ T $$ does not represent an optimal prefix code for $$ C $$.
Then there exists an optimal tree $$ T'' $$ such that $$ B(T'') < B(T) $$.
WLOG (by Lemma 16.2), $$ T'' $$ has $$ x $$ and $$ y $$ as siblings.
Let $$ T''' $$ be the tree $$ T'' $$ witht he common parent of $$ x $$ and $$ y $$ replaced by a leaf $$ z $$ with frequency $$ z.freq = x.freq + y.freq $$.
Then

$$
\begin{align*}
    B(T''') &= B(T'') - x.freq - y.freq \\
            &< B(T) - x.freq - y.freq \\
            &= B(T')
\end{align*}
$$

yielding a contradiction to the assumption that $$ T' $$ represents an optimal prefix code for $$ C' $$.
Thus, $$ T $$ must represent an optimal prefix code for the alphabet $$ C $$. $$ \blacksquare $$

### Theorem 16.4

Procedure HUFFMAN produces an optimal prefix code.

**Proof**  
Immediate from Lemmas 16.2 and 16.3. $$ \blacksquare $$

## 16.4 Matroids and greedy methods

### Matroids

A matroid is an ordered pair $$ M = (S, \ \mathcal{I}) $$ satisfying the following conditions.

1. $$ S $$ is a finite set.
2. $$ \mathcal{I} $$ is a nonempty family of subsets of $$ S $$, called the independent subsets of $$ S $$, such that if $$ B \in \mathcal{I} $$ and $$ A \subseteq B $$, then $$ A \in \mathcal{I} $$.
We say that $$ \mathcal{I} $$ is hereditary if it satisfies this property.
Note that the empty set $$ \emptyset $$ is necessarily a member of $$ \mathcal{I} $$.
3. If $$ A \in \mathcal{I} $$, and $$ |A| < |B| $$, then there exists some element $$ x \in B - A $$ such that $$ A \cup \{ x \} \in \mathcal{I} $$.
We say that $$ M $$ satisfies the exchange property.