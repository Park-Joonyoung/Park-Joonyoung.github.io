---
title: 16. Greedy Algorithms
# description: Short summary of the post
date: 2024-12-07 16:04
categories: [Computer Science, Algorithm]
tags: [greedy-algorithm, activity-selection-problem, greedy-choice-property, optimal-substructure]     # TAG names should always be lowercase
math: true
pin: false
---

A greedy algorithm always makes the choice that looks best at the moment.
It makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution.

## 16.1 An activity-selection problem

Suppose we have a set $$ S = \{ a_1, \ a_2, \dots, \ a_n \} $$ of $$ n $$ proposed activities that wish to use a resource.
Each activity $$ a_i $$ has a start time $$ s_i $$ and a finish time $$ f_i $$, where $$ 0 \le s_i < f_i < \infty $$.
If selected, activity $$ a_i $$ takes place during the half-open time interval $$ [s_i, \ f_i) $$.
Activities $$ a_i $$ and $$ a_j $$ are compatible if the intervals $$ [s_i, \ f_i) $$ and $$ [s_j, \ f_j) $$ do not overlap.
In the activity-selection problem, we wish to select a maximum-size subset of mutually compatible activities.
Assume that the activities are sorted in monotonically increasing order of finish time:

$$
\begin{align*}
    f_1 \le f_2 \le \dots \le f_n
\end{align*}
\label{eq:1}
\tag{16.1}
$$

### The optimal substructure of the activity-selection problem

Denote by $$ S_{ij} $$ the set of activities that start after activity $$ a_i $$ finishes and that finish before activity $$ a_j $$ starts.
Suppose $$ A_{ij} $$ is a maximum set of mutually compatible activities in $$ S_{ij} $$, which includes some activity $$ a_k $$.
We assume that $$ a_k $$ is included in an optimal solution.
Let $$ A_{ik} = A_{ij} \cap S_{ik} $$ and $$ A_{kj} = A_{ij} \cap S_{kj} $$, then it turns out that $$ A_{ij} = A_{ik} \cup \{ a_k \} \cup A_{kj} $$.
Therefore, $$ A_{ij} $$ consists of $$ |A_{ij}| = |A_{ik}| + |A_{kj}| + 1 $$ activities.

The cut-and-paste argument shows that the optimal solution $$ A_{ij} $$ includes optimal solutions to the two subproblems for $$ S_{ik} $$ and $$ S_{kj} $$.
Suppose there is a set $$ A'_{kj} $$ of mutually compatible activities in $$ S_{kj} $$ where $$ |A'_{kj}| > |A_{kj}| $$, then $$ |A_{ik}| + |A'_{kj}| + 1 > |A_{ik}| + |A_{kj}| + 1 = |A_{ij}| $$ holds, which contradicts the assumption that $$ A_{ij} $$ is an optimal solution.
A symmetric argument applies to the activities in $$ S_{ik} $$.

This way of characterizing optimal substructure suggests that we might solve the activity-selection problem by dynamic programming.
Denote the size of an optimal solution for the set $$ S_{ij} $$ by $$ c[i, \ j] $$, then we would have the recurrence

$$
\begin{align*}
    c[i, \ j] =
    \begin{cases}
        \displaystyle 0                                                     & \text{if } S_{ij} = \emptyset \\
        \displaystyle \max_{a_k \in S_{ij}} \{ c[i, \ k] + c[k, \ j] + 1 \} & \text{if } S_{ij} \neq \emptyset
    \end{cases}
\end{align*}
\label{eq:2}
\tag{16.2}
$$

We could then develop a recursive algorithm and memoize it, or we could work bottom-up and fill in table entries as we go along.
However, we would be overlooking another important characteristic of the activity-selection problem.

### Making the greedy choice

Intuition suggest that we should choose an activity that leaves the resource available for as many other activities as possible.
Now, of the activities we end up choosing, one of them must be the first one to finish.
Therefore, we choose the activity in $$ S $$ with the earliest finish time, since that would leave the resource available for as many of the activities that follow it as possible.
Because the activities are sorted in monotonically increasing order by finish time, the greedy choice is activity $$ a_1 $$.

If we make the greedy choice, we have only one remaining subproblem to solve: finding activities that start after $$ a_1 $$ finishes.
We have that $$ s_1 < f_1 $$, and $$ f_1 $$ is the earliest finish time of any activity, and therefore no activity can have a finish time less than or equal to $$ s_1 $$.
Thus, all activities that are compatible with activity $$ a_1 $$ must start after $$ a_1 $$ finishes.  
Furthermore, we have already established that the activity-selection problem exhibits optimal substructure.
Let $$ S_k = \{ a_i \in S : s_i \ge f_k \} $$ be the set of activities that start after activity $$ a_k $$ finishes.
If we make the greedy choice of activity $$ a_1 $$, then $$ S_1 $$ remains as the only subproblem to solve.
(We sometimes refer to the sets $$ S_k $$ as subproblems rather than as just sets of activities.)
Optimal substructure tells us that if $$ a_1 $$ is in the optimal solution, then an optimal solution to the original problem consists of activity $$ a_1 $$ and all the activities in an optimal solution to the subproblem $$ S_1 $$.

### Theorem 16.1

Consider any nonempty subproblem $$ S_k $$, and let $$ a_m $$ be an activity in $$ S_k $$ with the earliest finish time.
Then $$ a_m $$ is included in some maximum-size subset of mutually compatible activities of $$ S_k $$.

**Proof**  
Let $$ A_k $$ be a maximum-size subset of mutually compatible activities in $$ S_k $$, and let $$ a_j $$ be the activity in $$ A_k $$ with the earliest finish time.
If $$ a_j = a_m $$, we are done.
If $$ a_j \neq a_m $$, let the set $$ A'_k = A_k - \{ a_j \} \cup \{ a_m \} $$ be $$ A_k $$ but substituting $$ a_m $$ for $$ a_j $$.
The activities in $$ A'_k $$ are disjoint, which follows because the activities in $$ A_k $$ are disjoint, $$ a_j $$ is the first activity in $$ A_k $$ to finish, and $$ f_m \le f_j $$.
Since $$ |A'_k| = |A_k| $$, we conclude that $$ |A'_k| $$ is a maximum-size subset of mutually compatible activities of $$ S_k $$, and it includes $$ a_m $$. $$ \blacksquare $$

We can repeatedly choose the activity that finishes first, keep only the activities compatible with this activity, and repeat until no activities remain.
An algorithm to solve the activity-selection problem does not need to work bottom-up, like a table-based dynamic-programming algorithm.
Instead, it can work top-down, choosing an activity to put into the optimal solution and then solving the subproblem of choosing activities from those that are compatible with those already chosen.
Greedy algorithms typically have this top-down design: make a choice and then solve a subproblem, rather than the bottom-up technique of solving subproblems before making a choice.

### A recursive greedy algorithm

>RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$)  
>&nbsp;1&nbsp; $$ m = k + 1 $$  
>&nbsp;2&nbsp; while $$ m \le n $$ and $$ s[m] < f[k] $$  
>&nbsp;3&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m = m + 1 $$  
>&nbsp;4&nbsp; if $$ m \le n $$  
>&nbsp;5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ \{ a_m \} \ \cup $$ RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$)  
>&nbsp;6&nbsp; else return $$ \emptyset $$

In a given recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$), the while loop of lines 2–3 looks for the first activity in $$ S_k $$ to finish.
The loop examines $$ a_{k + 1}, \ a_{k + 2}, \dots , \ a_{n} $$, until it finds the first activity $$ a_m $$ that is compatible with $$ a_k $$; such an activity has $$ s_m \ge f_k $$.
If the loop terminates because it finds such an activity, line 5 returns the union of $$ \{ a_m \} $$ and the maximum-size subset of $$ S_m $$ returned by the recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$).
Alternatively, the loop may terminate because $$ m > n $$, in which case we have examined all activities in $$ S_k $$ without finding one that is compatible with $$ a_k $$.
In this case, $$ S_k = \emptyset $$, and so the procedure returns $$ \emptyset $$ in line 6.

Assuming that the activities have already been sorted by finish times, the running time of the call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ 0, \ n $$) is $$ \Theta(n) $$, which we can see as follows.
Over all recursive calls, each activity is examined exactly once in the while loop test of line 2.
In particular, activity $$ a_i $$ is examined in the last call made in which $$ k < i $$.

### An iterative greedy algorithm

>GREEDY-ACTIVITY-SELECTOR($$ s, \ f $$)  
>&nbsp;1&nbsp; $$ n = s.length $$  
>&nbsp;2&nbsp; $$ A = \{ a_1 \} $$  
>&nbsp;3&nbsp; $$ k = 1 $$  
>&nbsp;4&nbsp; for $$ m = 2 $$ to $$ n $$  
>&nbsp;5&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ s[m] \ge f[k] $$  
>&nbsp;6&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ A = A \cup \{ a_m \} $$  
>&nbsp;7&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ k = m $$  
>&nbsp;8&nbsp; return $$ A $$

The variable $$ k $$ indexes the most recent addition to $$ A $$, corresponding to the activity $$ a_k $$ in the recursive version.
Since we consider the activities in order of monotonically increasing finish time, $$ f_k $$ is always the maximum finish time of any activity in $$ A $$.
That is,

$$
\begin{align*}
    f_k = \max \{ f_i : a_i \in A \}
\end{align*}
\label{eq:3}
\tag{16.3}
$$

Lines 2–3 select activity $$ a_1 $$, initialize $$ A $$ to contain just this activity, and initialize $$ k $$ to index this activity.
The for loop of lines 4–7 finds the earliest activity in $$ S_k $$ to finish.
The loop considers each activity $$ a_m $$ in turn and adds $$ a_m $$ to $$ A $$ if it is compatible with all previously selected activities.
To see whether activity $$ a_m $$ is compatible with every activity in $$ A $$, it suffices to by equation \eqref{eq:3} to check that $$ s_m \ge f_k $$.
If activity $$ a_m $$ is compatible, then lines 6–7 add activity $$ a_m $$ to $$ A $$ and set $$ k $$ to $$ m $$.  
Like the recursive version, GREEDY-ACTIVITY-SELECTOR schedules a set of $$ n $$ activities in $$ \Theta(n) $$ time, assuming that the activities were already sorted initially by their finish times.

## 16.2 Elements of the greedy strategy

A greedy algorithm obtains an optimal solution to a solution to a problem by making a sequence of choices.
At each decision point, the algorithm makes the choice that seems best at the moment.
This heuristic strategy does not always produce an optimal solution, but sometimes, it does.
We design greedy algorithms according to the following sequence of steps:

1. Cast the optimization problem as one in which we make a choice and are left with one subproblem to solve.
2. Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.
3. Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if we combine an optimal solution to the subproblem with the greedy choice we have made, we arrive at an optimal solution to the original problem.

There are two key ingredients that the problem can be solved by using a greedy algorithm: the greedy-choice property and optimal substructure.

### Greedy-choice property

The greedy-choice property is that we can assemble a globally optimal solution by making locally optimal greedy choices.
In other words, when we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems.  
Here is where greedy algorithms differ from dynamic programming.
In dynamic programming, the choice we make depends on the solutions to subproblems.
Consequently, we typically solve dynamic-programming problems in a bottom-up manner, progressing from smaller subproblems to larger ones.
In a greedy algorithm, we make whatever choice seems best at the moment and then solve the subproblem that remains.
The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems.
A dynamic programming algorithm proceeds bottom up, whereas a greedy strategy usually progresses in a top-down fashion, making one greedy choice after another, reducing each given problem instance to a smaller one.

## Optimal substructure

A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solution to subproblems.
This property is a key ingredient of assessing the applicability of dynamic programming as well as greedy algorithms.
As an example of optimal substructure in Section 16.1, if an optimal solution to subproblem $$ S_{ij} $$ includes an activity $$ a_k $$, then it must also contain optimal solutions to the subproblems $$ S_{ik} $$ and $$ S_{kj} $$.
Based on this observation of optimal substructure, we were able to devise the recurrence \eqref{eq:2} that described the value of an optimal solution.  
We usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms.
We can assume that we arrived at a subproblem by having made the greedy choice in the original problem.
All we really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem.
This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step producese an optimal solution.

## 16.3 Huffman codes

Suppose we have a 100,000-character data file that we wish to store compactly.
We observe that the characters in the file occur with the frequencies given as below. That is, only 6 different characters appear, and the character **a** occurs 45,000 times.

| |**a**|**b**|**c**|**d**|**e**|**f**|
|---|:---:|:---:|:---:|:---:|:---:|:---:|
|Frequency (in thousands)|45|13|12|6|9|5|
|Fixed-length codeword|000|001|010|011|100|101|
|Variable-length codeword|0|101|100|111|1101|1100|

Here, we consider the problem of designing a binary character code (or code for short) in which each character is represented by a unique binary string, which is called a codeword.
If we use a fixed-length codeword, we need 3 bits to represent 6 characters.
This method requires 300,000 bits to code the entire file.  
A variable-length code can do considerably better that a fixed-length code, by giving frequent characters short codewords and infrequent characters long codewords.
The table above shows such a code; here the 1-bit string 0 represents **a**, and the 4-bit string 1100 represents **f**.
This code requires

$$
\begin{align*}
    (45 \cdot 1 + 13 \cdot 3 + 12 \cdot 3 + 16 \cdot 3 + 9 \cdot 4 + 5 \cdot 4) \cdot 1000 = 224,000 \text{ bits}
\end{align*}
$$

to represent the file, a savings of approximately 25%.
In fact, this is an optimal character code for this file.

### Prefix codes

We consider only codes in which no codewords is also a prefix of some other codeword.
Such codes are called prefix codes.
A prefix code can always achieve the optimal data compression among any character code, and so we suffer no loss of generality by restricting our attention to prefix codes.

Prefix codes are desirable because they simplify decoding.
Since no codeword is a prefix of any other, the codeword that begins an encoded file is unambiguous.
For example, the string 001011101 parses uniquely as $$ 0 \cdot 0 \cdot 101 \cdot 1101 $$, which decodes to **aabe**.

A binary tree whose leaves are the given characters provides a convenient representation for the prefix code.
We interpret the binary codeword for a character as the simple path from the root to that character, where 0 means "go to the left child" and 1 means "go to the right child."
Figure 16.1 shows the trees for the two codes of our example.
Note that these are not binary search trees, since the leaves need not appear in sorted order and internal nodes do not contain character keys.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.1.png){: width="700" height="400" }
_**Figure 16.1** **(a)** The tree corresponding to the fixed-length code. **(b)** The tree corresponding to the optimal prefix code._
